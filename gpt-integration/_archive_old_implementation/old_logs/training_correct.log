/data/miniforge3/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading ARC dataset...
Loaded 10 train problems
Loaded 5 eval problems
Total training steps: 500
Initializing model...
Loading meta-llama/Llama-3.1-8B-Instruct...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:10,  3.47s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:04<00:04,  2.32s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:06<00:01,  1.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.25s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.65s/it]
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: joungju257 (joungju257-gwangju-institute-of-science-and-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/ubuntu/TinyRecursiveModels/gpt-integration/wandb/run-20251010_234449-ecez1csb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run phase1_trm_correct
wandb: â­ï¸ View project at https://wandb.ai/joungju257-gwangju-institute-of-science-and-technology/arc-trm-hybrid
wandb: ðŸš€ View run at https://wandb.ai/joungju257-gwangju-institute-of-science-and-technology/arc-trm-hybrid/runs/ecez1csb
âœ“ LLaMA-8B frozen
âœ“ Hybrid TRM Model initialized
Trainable parameters: 11,022,336

Epoch 1/50
Training:   0%|          | 0/10 [00:00<?, ?it/s]Traceback (most recent call last):
Traceback (most recent call last):
  File "/data/miniforge3/lib/python3.10/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
Traceback (most recent call last):
  File "/data/miniforge3/lib/python3.10/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
Traceback (most recent call last):
  File "/data/miniforge3/lib/python3.10/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/data/miniforge3/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/data/miniforge3/lib/python3.10/shutil.py", line 681, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/data/miniforge3/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs00000000b8c4bd88000015ae'
  File "/data/miniforge3/lib/python3.10/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/data/miniforge3/lib/python3.10/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/data/miniforge3/lib/python3.10/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/data/miniforge3/lib/python3.10/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/data/miniforge3/lib/python3.10/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/data/miniforge3/lib/python3.10/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/data/miniforge3/lib/python3.10/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/data/miniforge3/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/data/miniforge3/lib/python3.10/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/data/miniforge3/lib/python3.10/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/data/miniforge3/lib/python3.10/shutil.py", line 681, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/data/miniforge3/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/data/miniforge3/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/data/miniforge3/lib/python3.10/shutil.py", line 681, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/data/miniforge3/lib/python3.10/shutil.py", line 681, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/data/miniforge3/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/data/miniforge3/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/data/miniforge3/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs00000000c8c262cd000015af'
OSError: [Errno 16] Device or resource busy: '.nfs00000000c0c4d810000015b1'
OSError: [Errno 16] Device or resource busy: '.nfs00000000c8c70e87000015b0'
Training:   0%|          | 0/10 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/ubuntu/TinyRecursiveModels/gpt-integration/training/train_trm_hybrid.py", line 406, in <module>
    main()
  File "/home/ubuntu/TinyRecursiveModels/gpt-integration/training/train_trm_hybrid.py", line 402, in main
    train(config)
  File "/home/ubuntu/TinyRecursiveModels/gpt-integration/training/train_trm_hybrid.py", line 301, in train
    carry, loss, metrics = train_step(
  File "/home/ubuntu/TinyRecursiveModels/gpt-integration/training/train_trm_hybrid.py", line 111, in train_step
    carry = model.inner.reset_carry(reset_flag, carry)
  File "/home/ubuntu/TinyRecursiveModels/gpt-integration/models/trm_hybrid.py", line 213, in reset_carry
    z_H=torch.where(reset_flag.view(-1, 1, 1), self.H_init, carry.z_H),
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Traceback (most recent call last):
  File "/home/ubuntu/TinyRecursiveModels/gpt-integration/training/train_trm_hybrid.py", line 406, in <module>
    main()
  File "/home/ubuntu/TinyRecursiveModels/gpt-integration/training/train_trm_hybrid.py", line 402, in main
    train(config)
  File "/home/ubuntu/TinyRecursiveModels/gpt-integration/training/train_trm_hybrid.py", line 301, in train
    carry, loss, metrics = train_step(
  File "/home/ubuntu/TinyRecursiveModels/gpt-integration/training/train_trm_hybrid.py", line 111, in train_step
    carry = model.inner.reset_carry(reset_flag, carry)
  File "/home/ubuntu/TinyRecursiveModels/gpt-integration/models/trm_hybrid.py", line 213, in reset_carry
    z_H=torch.where(reset_flag.view(-1, 1, 1), self.H_init, carry.z_H),
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mphase1_trm_correct[0m at: [34mhttps://wandb.ai/joungju257-gwangju-institute-of-science-and-technology/arc-trm-hybrid/runs/ecez1csb[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251010_234449-ecez1csb/logs[0m
