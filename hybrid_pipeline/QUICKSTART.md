# ğŸš€ Hybrid Pipeline ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

**ì™„ì „íˆ ì¬êµ¬ì„±ëœ LLaMA + TRM í†µí•© íŒŒì´í”„ë¼ì¸**

## âœ… ì™„ë£Œëœ ì‘ì—…

### 1. gpt_oss_port ëª¨ë“ˆ (LLM + Planner + Verifier)

```
gpt_oss_port/
â”œâ”€â”€ llm.py                 # TextReasoningModule (gpt_integration ëŒ€ì²´)
â”œâ”€â”€ planner.py             # ARCPlanner (multi-attempt reasoning)
â”œâ”€â”€ verifier.py            # GridVerifier (validation + feedback)
â”œâ”€â”€ grid_utils.py          # Grid utilities
â”œâ”€â”€ dataset_access.py      # Dataset wrapper
â”œâ”€â”€ run_baseline.py        # CLI baseline runner
â””â”€â”€ tests/
    â”œâ”€â”€ test_planner.py    # âœ… í…ŒìŠ¤íŠ¸ í¬í•¨
    â””â”€â”€ test_grid_utils.py # âœ… í…ŒìŠ¤íŠ¸ í¬í•¨
```

**í•µì‹¬ ë³€ê²½**: `gpt_integration.models.text_reasoning` â†’ `gpt_oss_port.llm`

### 2. adapters ëª¨ë“ˆ (ë¶„ë¦¬ ë° ì •ë¦¬)

```
adapters/
â”œâ”€â”€ text_to_latent.py      # TextToLatentAdapterë§Œ í¬í•¨
â”œâ”€â”€ latent_to_text.py      # LatentToTextAdapter (ë¶„ë¦¬ë¨)
â”œâ”€â”€ feedback_formatter.py  # ì •ë¦¬ë¨
â””â”€â”€ tests/
    â””â”€â”€ test_projection.py # âœ… import ê²½ë¡œ ìˆ˜ì •ë¨
```

### 3. run_joint_training.py (ì „ë©´ ê°œí¸)

**ì£¼ìš” ë³€ê²½**:
- âœ… `@dataclass` ê¸°ë°˜ `JointModelConfig`
- âœ… `asdict()` ì‚¬ìš©í•´ ì§ë ¬í™”
- âœ… `gpt_oss_port.llm` import (gpt_integration ì œê±°)
- âœ… `ARCPlanner` + `GridVerifier` í†µí•©
- âœ… TRM carry ì§ì ‘ ìƒì„± (ëª¨ë¸ forward ì‚¬ìš©)
- âœ… wandb ë¡œê¹… ê°•í™” (loss, exact_match, avg_attempts, avg_trm_steps, reasoning_length)

### 4. experiments ìŠ¤í¬ë¦½íŠ¸/ì„¤ì •

- âœ… `config_joint.yaml` - JointModelConfigì™€ ì¼ì¹˜
- âœ… `run_joint_training.sh` - CLI ì¸ì ì „ë‹¬
- âœ… `run_baseline.sh` - LLM-only baseline

### 5. TRM pretrain ìŠ¤í¬ë¦½íŠ¸

- âœ… `train_trm.py` - PuzzleDataset ì •í™•íˆ ì‚¬ìš©
- âœ… `eval_trm.py` - ë©”íƒ€ë°ì´í„° ë¡œë“œ ê²€ì¦

### 6. ë¬¸ì„œ

- âœ… `README.md` - ì „ì²´ êµ¬ì¡° ì„¤ëª…
- âœ… `QUICKSTART.md` - ì´ íŒŒì¼

### 7. ìµœì¢… ê²€ì¦

- âœ… `python -m compileall` í†µê³¼
- âœ… ëª¨ë“  import ê²½ë¡œ ì •ë¦¬
- âœ… gpt_integration ì˜ì¡´ì„± ì™„ì „ ì œê±°

---

## ğŸ¯ ì‹¤í–‰ ìˆœì„œ

### ğŸš€ ë¹ ë¥¸ ì‹¤í–‰ (Quick Start)

**TRM checkpointì´ ì´ë¯¸ ìˆë‹¤ë©´ ë°”ë¡œ ì‹œì‘ ê°€ëŠ¥:**

```bash
# 1. Quick eval (10 samples) - í’ˆì§ˆ í™•ì¸
./run_trm_eval.sh \
    /data/trm/pretrain/checkpoint_step_5000.pt \
    /data/arc/processed \
    cuda:0 \
    --num_samples 10

# 2. ê´œì°®ìœ¼ë©´ ë°”ë¡œ joint training âš¡
./run_joint_training.sh \
    --data_path /data/arc/processed \
    --output_dir /data/trm/joint_training \
    --trm_checkpoint /data/trm/pretrain/checkpoint_step_5000.pt \
    --device cuda:0 \
    --max_attempts 16
```

**ì¥ì **: TRM pretrain (3ì¼) ìƒëµ ê°€ëŠ¥

---

### **ë‹¨ê³„ 1: ë°ì´í„° ì¤€ë¹„**

```bash
cd /home/ubuntu/TinyRecursiveModels/dataset

python build_arc_dataset.py \
    --input_file_prefix /path/to/arc/data \
    --output_dir /data/arc/processed \
    --subsets training evaluation \
    --test_set_name evaluation \
    --num_aug 100
```

**ì¶œë ¥**: `/data/arc/processed/` (train/test splits)

---

### **ë‹¨ê³„ 2: Baseline ì‹¤í–‰** (ì„ íƒ ì‚¬í•­)

```bash
cd /home/ubuntu/TinyRecursiveModels/hybrid_pipeline/experiments

./run_baseline.sh \
    /path/to/arc_agi \
    /data/trm/baseline_results.json \
    evaluation \
    3
```

**ì„¤ëª…**: LLM-only baseline (TRM ì—†ì´), dataset_access ì‚¬ìš©
**ì…ë ¥**: ì›ë³¸ ARC JSON íŒŒì¼ ê²½ë¡œ (e.g., `/path/to/arc_agi`)
**ì¶œë ¥**: `/data/trm/baseline_results.json`

**ì°¸ê³ **: Baselineì€ ì›ë³¸ ARC JSON íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤ (ì˜ˆ: `arc_agi_evaluation_challenges.json`)

---

### **ë‹¨ê³„ 3: TRM ì‚¬ì „í•™ìŠµ** (ê¶Œì¥)

```bash
./run_trm_pretrain.sh \
    /data/arc/processed \
    /data/trm/pretrain \
    cuda:0
```

**ì„¤ëª…**: TRMì„ ARC grid ìƒì„± taskì— ì‚¬ì „í•™ìŠµ
**ì¶œë ¥**: `/data/trm/pretrain/checkpoint_step_*.pt`
**WandB**: `arc-trm-pretrain`

---

### **ë‹¨ê³„ 4: TRM í‰ê°€**

```bash
./run_trm_eval.sh \
    /data/trm/pretrain/checkpoint_step_5000.pt \
    /data/arc/processed \
    cuda:0
```

**ì„¤ëª…**: ì‚¬ì „í•™ìŠµëœ TRM ì„±ëŠ¥ ê²€ì¦
**ì¶œë ¥**: `/data/trm/pretrain/eval_results.json`

---

### **ë‹¨ê³„ 5: ì¡°ì¸íŠ¸ í•™ìŠµ** (LLaMA + TRM)

```bash
./run_joint_training.sh \
    --data_path /data/arc/processed \
    --output_dir /data/trm/joint_training \
    --trm_checkpoint /data/trm/pretrain/checkpoint_step_5000.pt \
    --device cuda:0 \
    --max_attempts 16 \
    --epochs 10 \
    --batch_size 1 \
    --lr 1e-4
```

**ì„¤ëª…**:
- LLaMA (frozen) + adapters (trainable) + TRM (trainable)
- ìµœëŒ€ 16íšŒ ìê¸°ìˆ˜ì • ì‹œë„
- Planner ê¸°ë°˜ multi-attempt ë£¨í”„

**ì¶œë ¥**: `/data/trm/joint_training/checkpoint_step_*.pt`
**WandB**: `arc-hybrid-joint`

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰

### 1. Adapter í…ŒìŠ¤íŠ¸

```bash
cd /home/ubuntu/TinyRecursiveModels/hybrid_pipeline/adapters/tests
python test_projection.py
```

**ì˜ˆìƒ ì¶œë ¥**:
```
====================================
Adapter Projection Tests
====================================
ğŸ§ª Testing TextToLatentAdapter...
  âœ… z_H shape: (2, 900, 512)...
ğŸ§ª Testing LatentToTextAdapter...
  âœ… Latent prefix shape: (2, 4096)...
====================================
âœ… All tests passed!
====================================
```

### 2. Planner í…ŒìŠ¤íŠ¸

```bash
cd ../../gpt_oss_port/tests
python test_planner.py
```

**ì˜ˆìƒ ì¶œë ¥**:
```
====================================
Planner Tests
====================================
ğŸ§ª Testing planner basic functionality...
  âœ… Basic functionality works
...
âœ… All planner tests passed!
====================================
```

### 3. Grid Utils í…ŒìŠ¤íŠ¸

```bash
python test_grid_utils.py
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### WandB í”„ë¡œì íŠ¸

1. **TRM Pretraining**: `arc-trm-pretrain`
   - loss, accuracy, avg_steps

2. **Joint Training**: `arc-hybrid-joint`
   - loss, exact_match, avg_attempts, avg_trm_steps, avg_reasoning_length

### ì£¼ìš” ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ì„¤ëª… |
|--------|------|
| `loss` | Cross-entropy loss |
| `exact_match` | ì •í™•í•œ grid ì¼ì¹˜ ë¹„ìœ¨ |
| `shape_match` | Grid í¬ê¸° ì¼ì¹˜ ë¹„ìœ¨ |
| `cell_accuracy` | Cell-level ì •í™•ë„ |
| `avg_attempts` | í‰ê·  ìê¸°ìˆ˜ì • ì‹œë„ íšŸìˆ˜ |
| `avg_trm_steps` | TRM ACT í‰ê·  ì¶”ë¡  ìŠ¤í… |
| `avg_reasoning_length` | LLaMA ì¶”ë¡  í…ìŠ¤íŠ¸ í‰ê·  ê¸¸ì´ |

---

## âš™ï¸ ì„¤ì •

### config_joint.yaml í¸ì§‘

```yaml
# Data
data_path: "/data/arc/processed"

# LLaMA
llama_model: "meta-llama/Llama-3.2-8B-Instruct"
llama_frozen: true  # ì¶”ì²œ

# TRM
trm_checkpoint: "/data/trm/pretrain/checkpoint_step_5000.pt"
trm_hidden_size: 512
trm_halt_max_steps: 16

# Training
batch_size: 1  # GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •
max_attempts: 16  # ìê¸°ìˆ˜ì • ìµœëŒ€ íšŸìˆ˜
epochs: 10
lr: 0.0001
```

---

## ğŸ› ë¬¸ì œ í•´ê²°

### 1. CUDA Out of Memory

```bash
# í•´ê²°ì±…
--batch_size 1
--max_attempts 8  # 16ì—ì„œ ê°ì†Œ
```

### 2. Import ì˜¤ë¥˜

```bash
# PYTHONPATH ì„¤ì •
export PYTHONPATH=/home/ubuntu/TinyRecursiveModels:$PYTHONPATH

# ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì—ì„œ sys.path í™•ì¸
python -c "import sys; print('\n'.join(sys.path))"
```

### 3. TRM í•™ìŠµ ì•ˆ ë¨

```bash
# ì‚¬ì „í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš© í•„ìˆ˜
--trm_checkpoint /data/trm/pretrain/checkpoint_step_5000.pt

# Learning rate í™•ì¸
--lr 1e-4  # ë„ˆë¬´ ë‚®ì§€ ì•Šì€ì§€ í™•ì¸
```

### 4. gpt_integration import ì˜¤ë¥˜

```bash
# âŒ ëª¨ë“  gpt_integration import ì œê±°ë¨
# âœ… gpt_oss_port.llm ì‚¬ìš©

# í™•ì¸
grep -r "gpt_integration" hybrid_pipeline/
# (ê²°ê³¼ ì—†ì–´ì•¼ í•¨)
```

---

## ğŸ“ íŒŒì¼ ìœ„ì¹˜ ì°¸ì¡°

| ì»´í¬ë„ŒíŠ¸ | ê²½ë¡œ |
|----------|------|
| ë°ì´í„° | `/data/arc/processed/` |
| TRM ì²´í¬í¬ì¸íŠ¸ | `/data/trm/pretrain/` |
| ì¡°ì¸íŠ¸ ì²´í¬í¬ì¸íŠ¸ | `/data/trm/joint_training/` |
| ë¡œê·¸ | `/data/trm/*/train_*.log` |
| ì†ŒìŠ¤ (ì°¸ì¡°) | `/home/ubuntu/TinyRecursiveModels/models/` |
| ë°ì´í„°ì…‹ (ì°¸ì¡°) | `/home/ubuntu/TinyRecursiveModels/dataset/` |

---

## ğŸ”‘ í•µì‹¬ ì°¨ì´ì  (ì´ì „ ë²„ì „ ëŒ€ë¹„)

| í•­ëª© | ì´ì „ | í˜„ì¬ |
|------|------|------|
| LLM ëª¨ë“ˆ | `gpt_integration.models.text_reasoning` | `gpt_oss_port.llm.TextReasoningModule` |
| Config | Class + `to_dict()` | `@dataclass` + `asdict()` |
| TRM ì‹¤í–‰ | ì§ì ‘ while ë£¨í”„ | `model.initial_carry()` + `model()` |
| Planner | ì—†ìŒ | `ARCPlanner` í´ë˜ìŠ¤ |
| Verifier | ì—†ìŒ | `GridVerifier` í´ë˜ìŠ¤ |
| í…ŒìŠ¤íŠ¸ | print ê¸°ë°˜ | assert ê¸°ë°˜ |

---

## ğŸ“š ë‹¤ìŒ ë‹¨ê³„

1. **ë°ì´í„° ì¤€ë¹„** â†’ ARC ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
2. **Baseline ì‹¤í–‰** (ì„ íƒ) â†’ LLM-only ì„±ëŠ¥ ì¸¡ì •
3. **TRM ì‚¬ì „í•™ìŠµ** â†’ Grid ìƒì„± ëŠ¥ë ¥ í•™ìŠµ
4. **TRM í‰ê°€** â†’ ì‚¬ì „í•™ìŠµ í’ˆì§ˆ í™•ì¸
5. **ì¡°ì¸íŠ¸ í•™ìŠµ** â†’ LLaMA + TRM í†µí•©
6. **ì„±ëŠ¥ ë¶„ì„** â†’ WandB ëŒ€ì‹œë³´ë“œ í™•ì¸

---

## âœ¨ ì£¼ìš” ê°œì„  ì‚¬í•­

1. âœ… **gpt_integration ì˜ì¡´ì„± ì™„ì „ ì œê±°**
2. âœ… **ëª¨ë“ˆí™”ëœ ì•„í‚¤í…ì²˜** (gpt_oss_port + adapters)
3. âœ… **dataclass ê¸°ë°˜ ì„¤ì •**
4. âœ… **í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸**
5. âœ… **wandb ë¡œê¹… ê°•í™”**
6. âœ… **compileall ê²€ì¦ í†µê³¼**

---

**ìƒíƒœ**: âœ… Production-ready
**Python íŒŒì¼**: 15ê°œ
**Shell ìŠ¤í¬ë¦½íŠ¸**: 4ê°œ
**í…ŒìŠ¤íŠ¸**: 3ê°œ ëª¨ë“ˆ (adapters, planner, grid_utils)

ğŸ‰ **ì¤€ë¹„ ì™„ë£Œ!** ìœ„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì„¸ìš”.
