# Hybrid LLaMA + TRM Pipeline for ARC-AGI

**ì™„ì „íˆ ì¬êµ¬ì„±ëœ hybrid pipeline - gpt_integration ì˜ì¡´ì„± ì œê±°**

## ğŸ“ êµ¬ì¡°

```
hybrid_pipeline/
â”œâ”€â”€ gpt_oss_port/              # GPT-OSS ë¡œì§ ëª¨ë“ˆí™” (LLM + planner + verifier)
â”‚   â”œâ”€â”€ llm.py                 # TextReasoningModule (LLaMA wrapper)
â”‚   â”œâ”€â”€ planner.py             # ARCPlanner (multi-attempt reasoning)
â”‚   â”œâ”€â”€ verifier.py            # GridVerifier (validation + feedback)
â”‚   â”œâ”€â”€ grid_utils.py          # Grid formatting utilities
â”‚   â”œâ”€â”€ dataset_access.py      # Wrapper for existing dataset/ module
â”‚   â”œâ”€â”€ run_baseline.py        # CLI baseline runner (LLM-only)
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ test_planner.py    # Planner unit tests
â”‚       â””â”€â”€ test_grid_utils.py # Grid utils tests
â”‚
â”œâ”€â”€ adapters/                  # LLMâ†”TRM interface
â”‚   â”œâ”€â”€ text_to_latent.py      # TextToLatentAdapter (LLaMA â†’ TRM)
â”‚   â”œâ”€â”€ latent_to_text.py      # LatentToTextAdapter (TRM â†’ LLaMA)
â”‚   â”œâ”€â”€ feedback_formatter.py  # Grid â†’ feedback text
â”‚   â””â”€â”€ tests/
â”‚       â””â”€â”€ test_projection.py # Adapter tests
â”‚
â”œâ”€â”€ trm_pretrain/              # TRM pretraining (single GPU)
â”‚   â”œâ”€â”€ train_trm.py           # Simplified pretrain script
â”‚   â””â”€â”€ eval_trm.py            # TRM evaluation
â”‚
â”œâ”€â”€ experiments/               # Joint training orchestration
â”‚   â”œâ”€â”€ run_joint_training.py  # Main joint training (LLaMA + TRM)
â”‚   â”œâ”€â”€ config_joint.yaml      # Configuration file
â”‚   â”œâ”€â”€ run_trm_pretrain.sh    # TRM pretrain script
â”‚   â”œâ”€â”€ run_trm_eval.sh        # TRM eval script
â”‚   â”œâ”€â”€ run_joint_training.sh  # Joint training script
â”‚   â””â”€â”€ run_baseline.sh        # Baseline script
â”‚
â””â”€â”€ docs/
    â””â”€â”€ README.md              # This file
```

## ğŸ”‘ í•µì‹¬ ì„¤ê³„ ì›ì¹™

### âœ… ì½”ë“œ ì¤‘ë³µ ì œê±°
- ëª¨ë“  TRM/dataset ì½”ë“œëŠ” ê¸°ì¡´ `/home/ubuntu/TinyRecursiveModels/models/` ë° `dataset/`ì—ì„œ import
- **gpt_integration ì˜ì¡´ì„± ì™„ì „ ì œê±°**

### âœ… ëª¨ë“ˆí™”
- `gpt_oss_port`: LLM ì¶”ë¡  + planning ë¡œì§
- `adapters`: LLMâ†”TRM ì¸í„°í˜ì´ìŠ¤
- `trm_pretrain`: TRM ì‚¬ì „í•™ìŠµ
- `experiments`: ì „ì²´ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

### âœ… dataclass ê¸°ë°˜ ì„¤ì •
- `JointModelConfig`: dataclassë¡œ ì •ì˜
- `asdict()` ì‚¬ìš©í•´ ì§ë ¬í™”

### âœ… ê·¸ë˜ë””ì–¸íŠ¸ íë¦„
- `.detach()` ì—†ìŒ
- Adapter + TRM í•™ìŠµ ê°€ëŠ¥
- LLaMA ë™ê²° (ì„ íƒì )

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ë°ì´í„° ì¤€ë¹„

```bash
cd /home/ubuntu/TinyRecursiveModels/dataset
python build_arc_dataset.py \
    --input_file_prefix /path/to/arc \
    --output_dir /data/arc/processed \
    --subsets training evaluation \
    --test_set_name evaluation \
    --num_aug 100
```

### 2. Baseline ì‹¤í–‰ (LLM-only)

```bash
cd /home/ubuntu/TinyRecursiveModels/hybrid_pipeline/experiments
./run_baseline.sh \
    /path/to/arc_agi \
    /data/trm/baseline_results.json \
    evaluation \
    3
```

**Note**: Baseline uses original ARC JSON files via dataset_access, not preprocessed data.

### 3. TRM ì‚¬ì „í•™ìŠµ

```bash
./run_trm_pretrain.sh \
    /data/arc/processed \
    /data/trm/pretrain \
    cuda:0
```

### 4. TRM í‰ê°€

```bash
./run_trm_eval.sh \
    /data/trm/pretrain/checkpoint_step_5000.pt \
    /data/arc/processed \
    cuda:0
```

### 5. ì¡°ì¸íŠ¸ í•™ìŠµ (LLaMA + TRM)

```bash
./run_joint_training.sh \
    --data_path /data/arc/processed \
    --output_dir /data/trm/joint_training \
    --trm_checkpoint /data/trm/pretrain/checkpoint_step_5000.pt \
    --device cuda:0 \
    --max_attempts 16
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# Adapter í…ŒìŠ¤íŠ¸
cd /home/ubuntu/TinyRecursiveModels/hybrid_pipeline/adapters/tests
python test_projection.py

# Planner í…ŒìŠ¤íŠ¸
cd ../../gpt_oss_port/tests
python test_planner.py

# Grid utils í…ŒìŠ¤íŠ¸
python test_grid_utils.py
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ (WandB)

- **TRM Pretraining**: `arc-trm-pretrain`
- **Joint Training**: `arc-hybrid-joint`

ë¡œê·¸ëœ ë©”íŠ¸ë¦­:
- `loss`: í‰ê·  ì†ì‹¤
- `exact_match`: ì •í™•í•œ ì¼ì¹˜ ë¹„ìœ¨
- `shape_match`: Grid í¬ê¸° ì¼ì¹˜ ë¹„ìœ¨
- `cell_accuracy`: Cell-level ì •í™•ë„
- `avg_attempts`: í‰ê·  ì‹œë„ íšŸìˆ˜
- `avg_trm_steps`: TRM í‰ê·  ì¶”ë¡  ìŠ¤í…
- `avg_reasoning_length`: í‰ê·  ì¶”ë¡  í…ìŠ¤íŠ¸ ê¸¸ì´

## ğŸ“ ì£¼ìš” ë³€ê²½ ì‚¬í•­ (ì´ì „ ë²„ì „ ëŒ€ë¹„)

| í•­ëª© | ì´ì „ | í˜„ì¬ |
|------|------|------|
| LLM ëª¨ë“ˆ | `gpt_integration.models.text_reasoning` | `gpt_oss_port.llm` |
| Planner | ì—†ìŒ | `gpt_oss_port.planner.ARCPlanner` |
| Verifier | ì—†ìŒ | `gpt_oss_port.verifier.GridVerifier` |
| Config | Class with `to_dict()` | `@dataclass` with `asdict()` |
| TRM carry | ì§ì ‘ ìƒì„± + while ë£¨í”„ | `model.initial_carry()` + `model()` |
| Import ê²½ë¡œ | `gpt_integration` | `gpt_oss_port` + `adapters` |

## ğŸ”§ ì„¤ì • íŒŒì¼

`experiments/config_joint.yaml` í¸ì§‘:

```yaml
# Data
data_path: "/data/arc/processed"

# LLaMA
llama_model: "meta-llama/Llama-3.2-8B-Instruct"
llama_frozen: true

# TRM
trm_checkpoint: "/data/trm/pretrain/checkpoint_step_5000.pt"
trm_hidden_size: 512
trm_halt_max_steps: 16

# Training
batch_size: 1
max_attempts: 16
epochs: 10
lr: 0.0001
```

## ğŸ“š API ì°¸ì¡°

### `gpt_oss_port.llm.TextReasoningModule`

```python
llm = TextReasoningModule(
    model_name="meta-llama/Llama-3.2-8B-Instruct",
    freeze=True,
    device="cuda"
)

# Generate with optional latent prefix
z_init, text = llm.generate_latent(
    problem_text="Solve this puzzle",
    latent_prefix=None,  # Optional [hidden_size] tensor
    max_length=128
)
```

### `gpt_oss_port.planner.ARCPlanner`

```python
planner = ARCPlanner(
    llm_module=llm,
    max_attempts=16
)

# Multi-attempt solving
results = planner.multi_attempt_solve(
    problem_description="...",
    verifier_fn=verifier_function
)
```

### `adapters.text_to_latent.TextToLatentAdapter`

```python
adapter = TextToLatentAdapter(
    llm_hidden_size=4096,
    trm_hidden_size=512,
    trm_seq_len=900
)

z_H, z_L = adapter(llm_hidden_state)  # [batch, llm_dim] â†’ [batch, seq, trm_dim]
```

## ğŸ› ë¬¸ì œ í•´ê²°

### CUDA OOM
- `--batch_size 1` ì‚¬ìš©
- `--max_attempts 8`ë¡œ ê°ì†Œ

### Import ì˜¤ë¥˜
- `sys.path`ì— `/home/ubuntu/TinyRecursiveModels` í¬í•¨ í™•ì¸
- `PYTHONPATH` ì„¤ì •

### TRM í•™ìŠµ ì•ˆ ë¨
- ì‚¬ì „í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©
- Learning rate í™•ì¸ (`--lr 1e-4`)

## ğŸ“ íŒŒì¼ ìœ„ì¹˜

| ì»´í¬ë„ŒíŠ¸ | ìœ„ì¹˜ |
|----------|------|
| ë°ì´í„° | `/data/arc/processed/` |
| TRM ì²´í¬í¬ì¸íŠ¸ | `/data/trm/pretrain/` |
| ì¡°ì¸íŠ¸ ì²´í¬í¬ì¸íŠ¸ | `/data/trm/joint_training/` |
| ë¡œê·¸ | `/data/trm/*/train_*.log` |

## âš ï¸ ì¤‘ìš” ì‚¬í•­

1. **ëª¨ë“  ê²½ë¡œëŠ” CLI ì¸ìë¡œ ì „ë‹¬ í•„ìˆ˜**
2. **gpt_integration ì˜ì¡´ì„± ì—†ìŒ**
3. **ê¸°ì¡´ models/ ë° dataset/ ì¬ì‚¬ìš©**
4. **compileall í†µê³¼ ê²€ì¦ ì™„ë£Œ**

---

**ìƒíƒœ**: âœ… Production-ready
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-13
**Python íŒŒì¼**: 18ê°œ
**Shell ìŠ¤í¬ë¦½íŠ¸**: 4ê°œ
