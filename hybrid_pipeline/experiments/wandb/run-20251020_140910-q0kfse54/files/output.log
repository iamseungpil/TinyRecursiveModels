
ðŸš€ Starting joint training...

ðŸ“– Epoch 1/10
Traceback (most recent call last):
  File "/home/ubuntu/TinyRecursiveModels/hybrid_pipeline/experiments/run_joint_training.py", line 1001, in <module>
    main()
  File "/home/ubuntu/TinyRecursiveModels/hybrid_pipeline/experiments/run_joint_training.py", line 997, in main
    train(config)
  File "/home/ubuntu/TinyRecursiveModels/hybrid_pipeline/experiments/run_joint_training.py", line 728, in train
    metrics = train_step(model, batch, optimizer, step, precomputed_embeddings)
  File "/home/ubuntu/TinyRecursiveModels/hybrid_pipeline/experiments/run_joint_training.py", line 614, in train_step
    latent_prefix_batch = model.latent_to_text(
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/TinyRecursiveModels/hybrid_pipeline/adapters/latent_to_text.py", line 132, in forward
    z_H_pooled = self.pool_H(z_H)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/TinyRecursiveModels/hybrid_pipeline/adapters/latent_to_text.py", line 38, in forward
    attn_scores = self.attention(z)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 must have the same dtype, but got BFloat16 and Float
Traceback (most recent call last):
  File "/home/ubuntu/TinyRecursiveModels/hybrid_pipeline/experiments/run_joint_training.py", line 1001, in <module>
    main()
  File "/home/ubuntu/TinyRecursiveModels/hybrid_pipeline/experiments/run_joint_training.py", line 997, in main
    train(config)
  File "/home/ubuntu/TinyRecursiveModels/hybrid_pipeline/experiments/run_joint_training.py", line 728, in train
    metrics = train_step(model, batch, optimizer, step, precomputed_embeddings)
  File "/home/ubuntu/TinyRecursiveModels/hybrid_pipeline/experiments/run_joint_training.py", line 614, in train_step
    latent_prefix_batch = model.latent_to_text(
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/TinyRecursiveModels/hybrid_pipeline/adapters/latent_to_text.py", line 132, in forward
    z_H_pooled = self.pool_H(z_H)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/TinyRecursiveModels/hybrid_pipeline/adapters/latent_to_text.py", line 38, in forward
    attn_scores = self.attention(z)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 must have the same dtype, but got BFloat16 and Float
